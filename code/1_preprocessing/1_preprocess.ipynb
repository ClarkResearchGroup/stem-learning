{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating data for training and testing\n",
    "\n",
    "This notebook will describe the process in generating training data for the FCN. There are 4 scripts in the \n",
    "preprocessing section of the code, each containing a set of functions. Description for each function resides in the scripts themeselves.\n",
    "\n",
    "`generate_augments.py` is a module that makes augmented images from an input image such as rotations flips, and coarse graining.\n",
    "\n",
    "`image_parse.py` contains functions to read and parse raw (labels or stem) images.\n",
    "\n",
    "`make_data.py` contains functions to generate variations of the images as well as create data that's readable for the FCN.\n",
    "\n",
    "`1_preprocess.py` is a script with the same code as in this notebook but can be run in terminal (after setting the parameters).\n",
    "\n",
    "\n",
    "In this example, we will generate training data from WSeTe simulated images. We will go through some functions and descirbe the meaning of the parameters needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start by importing the functions in `make_data.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from make_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folder Hierarchy ##\n",
    "Next we set the parameters. To understand some parameters, we must describe the folder hierarchy that is assumed. The hieararchy is as follows:\n",
    "\n",
    "input_dir:\n",
    "\n",
    "    data_dir_0:\n",
    "        input.ftype \n",
    "        label_l0.ftype\n",
    "        label_l2.ftype\n",
    "        ...\n",
    "        label_lm.ftype\n",
    "    data_dir_2:\n",
    "        (similar to data_dir_0)\n",
    "    ...\n",
    "    data_dir_n:\n",
    "    \n",
    "    parsed_dir_name:\n",
    "            test_00000.p\n",
    "            \n",
    "            train_pname1.p\n",
    "            train_pname2.p\n",
    "            ...\n",
    "            train_pnamek.p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `data_dir` folder contains its own raw input (stem) image along with its corresponding labels. There is one image corresponding to a set of labels. The `data_dir` directories can be labeled however we wish, but inside each directory, the files have a specific name structure:\n",
    "\n",
    "*input.ftype*: There must be a file labeled \"input.tiff\" (or .png or any other ftype). Some file types may not work, so converting to a functioning one (like tiff) may be necessary.\n",
    "\n",
    "*label_x.ftype*: The images with labels must start with \"label_\" followed by a label name. For example \"label_Se.tiff\". The filetype of the label images should be the same as the input image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters ##\n",
    "First, we need to provide the location of `input_dir`, the directory where all the data is.\n",
    "In this directory that contains a set of data directories, we need to specify which ones are for training and which are for the validation set. We denote these list of directories in `train_dirs` and `test_dir`, respectively. `test_dir` will have only one directory in a list.\n",
    "\n",
    "We also need a list of the label names, `label_list`, that are in each data directory \"\\[l0, l1, ..., lm\\]\". \n",
    "Note that for `label_list`, it need not contain a list of all the labels in the data directory folders. One just includes the set of labels they wish to train an FCN on. It can be a list of one element.\n",
    "\n",
    "Next, we specify what the name of the directory where we place all our train and test data `parsed_dir_name`.\n",
    "\n",
    "Finally, we specify what filetype we're working with `ftype`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir  = \"../../data/WSeTe/simulated/\"\n",
    "train_dirs = [\"0\", \"1\", \"2\"]\n",
    "test_dir   = [\"3\"]\n",
    "label_list = [\"Se\"]\n",
    "parsed_dir_name = 'parsed_label_Se'\n",
    "ftype = '.tiff'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define parameters of extracting images. If we were to imagine the raw image a sheet of dough, we are getting our training set by cutting out pieces from that sheet. To do this, we need to describe the shape of our cookie cutter, and where on the sheet we are cutting out the pieces.\n",
    "\n",
    "`l_shape`: the height and width of the images going into the FCN (the size of the cookie cutter)\n",
    "\n",
    "`stride`: when going through the raw image extracting images of size `l_shape` the stride says how many pixels to move over to the left and down to get the next image. if the stride is equal to the shape, then the set of extracted images have no overlapping pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_shape = (256,256)\n",
    "stride = (64,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing is performed by cutting a bunch of images from the raw data in various ways and pickling them in a number of pickled files. If our training set is small, we can just deal with one pickeled file by setting `one_pickle` to true. Otherwise, we can specify how many cut images we want in a training file, `tr_fsize` and in a test file `ts_fsize`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_pickle=False\n",
    "tr_fsize = 2000\n",
    "ts_fsize = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way we parse our label images is that we take in the label file, and set pixel values above `tol` to 1 and pixel values below `tol` to zero. This is set below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we made a lot of cutouts with little to no defects. we can filter these images out by setting `ones_percent` to a nonzero value. This value is the percent of pixels in a cut image that is `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones_percent = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we set all the parameters, we can run functions to generate training data. The first of these is `create_augments`. This function will take the input images in a data directory, and output augments of that image in a folder inside each `data_dir/` called `augments`. The types of augments are inversions, rotations, and down/up sampling. \n",
    "Hence for a single input image, the augments create 2*4*3 = 24 different input images. This is only run on the train images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_augments(input_dir, train_dirs, ftype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we make the data that will be placed in `parsed_dir_name` for the train data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_data(input_dir, train_dirs, label_list, l_shape, stride, ftype, parsed_dir_name=parsed_dir_name, \\\n",
    "          prefix=\"train\", AUG=True, tol=tol, ones_pcent=ones_percent, one_save=one_pickle, fsize=tr_fsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_data(input_dir, test_dir, label_list, l_shape, stride, ftype, parsed_dir_name=parsed_dir_name, \\\n",
    "          prefix=\"test\", AUG=False, tol=tol, ones_pcent=ones_percent, one_save=True, fsize=ts_fsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we want to check our data, we use the function `check_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "parsed_fn = input_dir + parsed_dir_name + \"/test_00000.p\"\n",
    "check_data(parsed_fn, l_shape=l_shape) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
