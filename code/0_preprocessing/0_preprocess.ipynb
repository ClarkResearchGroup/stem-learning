{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example script for creating data for training\n",
    "\n",
    "This notebook will describe the process in generating training data for the FCN. There are 2 scripts in the \n",
    "preprocessing section of the code, each containing a set of functions. Description for each function resides in the scripts themeselves.\n",
    "\n",
    "`make_data.py` contains functions to generate variations of the images as well as create data that's readable for the FCN.\n",
    "\n",
    "`image_parse.py` contains functions to read and parse raw (labels or stem) images.\n",
    "\n",
    "\n",
    "\n",
    "In this example, we will generate training data from WSeTe simulated images. We will go through some functions and descirbe the meaning of the parameters needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start by importing the functions in `make_data.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from make_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set the parameters. To understand some parameters, we must describe the folder hierarchy that is assumed. The hieararchy is as follows:\n",
    "\n",
    "input_dir:\n",
    "\n",
    "    data_dir_0:\n",
    "        input.ftype \n",
    "        label_l0.ftype\n",
    "        label_l2.ftype\n",
    "        ...\n",
    "        label_lm.ftype\n",
    "    data_dir_2:\n",
    "        (similar to data_dir_0)\n",
    "    ...\n",
    "    data_dir_n:\n",
    "    \n",
    "    parsed_dir_name:\n",
    "        test:\n",
    "            test_pname1.p\n",
    "            test_pname2.p\n",
    "            ...\n",
    "            test_pnamek.p\n",
    "        train:\n",
    "            train_pname1.p\n",
    "            train_pname2.p\n",
    "            ...\n",
    "            train_pnam3.p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `data_dir` folder contains its own raw input (stem) image along with its corresponding labels. There is one image corresponding to a label. \n",
    "The naming convention of the images MUST be as follows in the `data_dir`. \n",
    "\n",
    "(1) the input (stem) image must be called `input.ftype` where `ftype` is the format of the image (jpeg, tiff, png, etc.). \n",
    "\n",
    "(2) the label images must be called `label_L.ftype` where `L` is the name of the label (2Te, SeTe, vacancy, etc.), and `ftype` is the same format as `input.ftype`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to provide the locations and names of `input_dir`, the list of data directories `data_dirs`, the labels in the data directories `label_list`, and the name of the directory where the training data is going `parsed_dir_name`\n",
    "\n",
    "Note that for `label_list`, it need not contain a list of all the labels in the data directory folders. One just includes the set of labels they wish to train an FCN on. This is usually a list of one element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"../../data/WSeTe/simulated/\"\n",
    "data_dirs = [str(i) for i in range(4)]\n",
    "label_list = [\"2Te\"]\n",
    "parsed_dir_name='parsed_label_2Te'\n",
    "ftype = '.tiff'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define parameters of extracting images. If we were to imagine the raw image a sheet of dough, we are getting our training set by cutting out pieces from that sheet. To do this, we need to describe the shape of our cookie cutter, and where on the sheet we are cutting out the pieces.\n",
    "\n",
    "`l_shape`: the height and width of the images going into the FCN (the size of the cookie cutter)\n",
    "\n",
    "`stride`: when going through the raw image extracting images of size `l_shape` the stride says how many pixels to move over to the left and down to get the next image. if the stride is equal to the shape, then the set of extracted images have no overlapping pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_shape = (256,256)\n",
    "stride = (128,128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing is performed by cutting a bunch of images from the raw data in various ways and pickling them in a number of pickled files. If our training set is small, we can just deal with one pickeled file by setting `one_pickle` to true. Otherwise, we can specify how many cut images we want in a training batch, `tr_bs` and in a test batch `ts_bs`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_pickle=True\n",
    "tr_bs = 2000\n",
    "ts_bs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we made a lot of cutouts with little to no defects. we can filter this out by setting `ones_percent` to a nonzero value. This value is the percent of pixels that are \"on\" due to a defect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones_percent = .00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way we parse our label images is that we take in the label file, and set pixel values above `tol` to 1 and pixel values below `tol` to zero. This is set below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final parameter is a debugging tool to take a look at these cutouts to make sure that everything is in order. Only set `show_plots` to true to verify that training data is what we expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plots=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we set all the parameters, we can run functions to generate training data. The first of these is `create_augments`. This function will take the input images in a data directory, and output augments of that image in a folder inside `data_dir/` called `augments`. The types of augments are inversions, rotations, and down/up sampling. \n",
    "Hence for a single input image, the augments create 2*4*3 = 24 different input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating augments in 0\n",
      "creating augments in 1\n",
      "creating augments in 2\n",
      "creating augments in 3\n"
     ]
    }
   ],
   "source": [
    "create_augments(input_dir, data_dirs, ftype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we make the data that will be placed in `parsed_dir_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "mag1_rot0_orig\n",
      "mag2_rot1_orig\n",
      "mag2_rot0_flip\n",
      "mag0_rot0_orig\n",
      "mag2_rot2_orig\n",
      "mag2_rot3_flip\n",
      "mag2_rot0_orig\n",
      "mag0_rot3_flip\n",
      "mag2_rot2_flip\n",
      "mag0_rot1_orig\n",
      "mag2_rot3_orig\n",
      "mag0_rot1_flip\n",
      "mag2_rot1_flip\n",
      "mag1_rot2_flip\n",
      "mag0_rot3_orig\n",
      "mag1_rot1_flip\n",
      "mag0_rot2_orig\n",
      "mag1_rot1_orig\n",
      "mag1_rot3_orig\n",
      "mag0_rot2_flip\n",
      "mag0_rot0_flip\n",
      "mag1_rot3_flip\n",
      "mag1_rot0_flip\n",
      "mag1_rot2_orig\n",
      "1176 total examples\n",
      "1\n",
      "mag1_rot0_orig\n",
      "mag2_rot1_orig\n",
      "mag2_rot0_flip\n",
      "mag0_rot0_orig\n",
      "mag2_rot2_orig\n",
      "mag2_rot3_flip\n",
      "mag2_rot0_orig\n",
      "mag0_rot3_flip\n",
      "mag2_rot2_flip\n",
      "mag0_rot1_orig\n",
      "mag2_rot3_orig\n",
      "mag0_rot1_flip\n",
      "mag2_rot1_flip\n",
      "mag1_rot2_flip\n",
      "mag0_rot3_orig\n",
      "mag1_rot1_flip\n",
      "mag0_rot2_orig\n",
      "mag1_rot1_orig\n",
      "mag1_rot3_orig\n",
      "mag0_rot2_flip\n",
      "mag0_rot0_flip\n",
      "mag1_rot3_flip\n",
      "mag1_rot0_flip\n",
      "mag1_rot2_orig\n",
      "1176 total examples\n",
      "2\n",
      "mag1_rot0_orig\n",
      "mag2_rot1_orig\n",
      "mag2_rot0_flip\n",
      "mag0_rot0_orig\n",
      "mag2_rot2_orig\n",
      "mag2_rot3_flip\n",
      "mag2_rot0_orig\n",
      "mag0_rot3_flip\n",
      "mag2_rot2_flip\n",
      "mag0_rot1_orig\n",
      "mag2_rot3_orig\n",
      "mag0_rot1_flip\n",
      "mag2_rot1_flip\n",
      "mag1_rot2_flip\n",
      "mag0_rot3_orig\n",
      "mag1_rot1_flip\n",
      "mag0_rot2_orig\n",
      "mag1_rot1_orig\n",
      "mag1_rot3_orig\n",
      "mag0_rot2_flip\n",
      "mag0_rot0_flip\n",
      "mag1_rot3_flip\n",
      "mag1_rot0_flip\n",
      "mag1_rot2_orig\n",
      "1176 total examples\n",
      "3\n",
      "mag1_rot0_orig\n",
      "mag2_rot1_orig\n",
      "mag2_rot0_flip\n",
      "mag0_rot0_orig\n",
      "mag2_rot2_orig\n",
      "mag2_rot3_flip\n",
      "mag2_rot0_orig\n",
      "mag0_rot3_flip\n",
      "mag2_rot2_flip\n",
      "mag0_rot1_orig\n",
      "mag2_rot3_orig\n",
      "mag0_rot1_flip\n",
      "mag2_rot1_flip\n",
      "mag1_rot2_flip\n",
      "mag0_rot3_orig\n",
      "mag1_rot1_flip\n",
      "mag0_rot2_orig\n",
      "mag1_rot1_orig\n",
      "mag1_rot3_orig\n",
      "mag0_rot2_flip\n",
      "mag0_rot0_flip\n",
      "mag1_rot3_flip\n",
      "mag1_rot0_flip\n",
      "mag1_rot2_orig\n",
      "1176 total examples\n"
     ]
    }
   ],
   "source": [
    "make_data(input_dir, label_list, data_dirs, l_shape, stride, ftype,\\\n",
    "        parsed_dir_name=parsed_dir_name, tr_bs=tr_bs, ts_bs=ts_bs, ones_percent=ones_percent, \\\n",
    "        tol=tol, show_plots=show_plots, one_pickle=one_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we want to check our data, we use the function `check_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8ae1cb97161c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmake_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mparsed_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mparsed_dir_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/train/train_0.p\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcheck_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dropbox/Research/Clarksearch/stem/stem-learning/code/preprocessing/make_data.py\u001b[0m in \u001b[0;36mcheck_data\u001b[0;34m(parsed_fn, idx, l_shape)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbl\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mnb_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlbl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0mlbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlbl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_classes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lx' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from make_data import *\n",
    "parsed_fn = input_dir + parsed_dir_name + \"/train/train_0.p\"\n",
    "check_data(parsed_fn, l_shape=l_shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
