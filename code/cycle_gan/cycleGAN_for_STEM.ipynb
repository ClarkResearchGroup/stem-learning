{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# cycleGANs for STEM images\n",
    "\n",
    "In this section, we'll train a GAN to noise up a simulated image to look like an experimental image"
   ],
   "metadata": {
    "id": "I62chQLHhFzk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "id": "iq_MGv7sdeEI"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from make_dataset import parse_and_save_dir, load_train_data\r\n",
    "from models import unet_generator, discriminator, generator_resnet\r\n",
    "import tensorflow as tf\r\n",
    "from plotting_tools import *\r\n",
    "import pickle\r\n",
    "import time\r\n",
    "from glob import glob\r\n",
    "from IPython.display import clear_output"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hsC0wWlu7QM0",
    "outputId": "24183030-eb8b-44fa-a2d6-25b490963e80"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading in Images\r\n",
    "First, we load in large simulated image and experimental images, and cut them into 256x256 chunks. We then store the images in folders."
   ],
   "metadata": {
    "id": "aJk0uCGsjHXo"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "parent_dir = \"C:/Users/abidk/Dropbox/Development/programs/stem-learning/\"\r\n",
    "exp_dir_raw = parent_dir + \"data/cyclegan_data/RR_212/\"\r\n",
    "sim_dir_raw = parent_dir + \"dat/cyclegan_data/sim_image/\"\r\n",
    "\r\n",
    "exp_dir = parent_dir + \"data/cyclegan_data/exp_212_256/\"\r\n",
    "sim_dir = parent_dir + \"data/cyclegan_data/sim_256/\"\r\n",
    "\r\n",
    "fine_size, stride = 256, 256\r\n",
    "num_channels=1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parse_and_save_dir(exp_dir_raw, exp_dir, fine_size, stride)\r\n",
    "#parse_and_save_dir(sim_dir_raw, sim_dir, fine_size, stride, num_channels=num_channels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Construct Generator and Discriminator\r\n",
    "Here, we can choose between a unet generator or a resnet generator."
   ],
   "metadata": {
    "id": "mlshv8r171WP"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "generator_exp = unet_generator(num_channels, 1, \"instancenorm\")\r\n",
    "generator_sim = unet_generator(1, num_channels, \"instancenorm\")\r\n",
    "\r\n",
    "#generator_exp = generator_resnet(64, num_channels, 1)\r\n",
    "#generator_sim = generator_resnet(64, 1, num_channels)\r\n",
    "\r\n",
    "discriminator_sim = discriminator(\"instancenorm\", num_channels)\r\n",
    "discriminator_exp = discriminator(\"instancenorm\", 1)"
   ],
   "outputs": [],
   "metadata": {
    "id": "s6bX45FX75l3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lr0 = 2e-3\r\n",
    "generator_exp_optimizer = tf.keras.optimizers.Adam(lr0)\r\n",
    "generator_sim_optimizer = tf.keras.optimizers.Adam(lr0)\r\n",
    "\r\n",
    "discriminator_sim_optimizer = tf.keras.optimizers.Adam(lr0)\r\n",
    "discriminator_exp_optimizer = tf.keras.optimizers.Adam(lr0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss Functions and Accuracies"
   ],
   "metadata": {
    "id": "U14mcm_gB6jL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "LAMBDA = 10\r\n",
    "#loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\n",
    "loss_obj = tf.keras.losses.MeanSquaredError()\r\n",
    "\r\n",
    "def discriminator_loss(real, fake):\r\n",
    "    real_loss = loss_obj( tf.ones_like(real), real)\r\n",
    "    fake_loss = loss_obj(tf.zeros_like(fake), fake)\r\n",
    "    total_disc_loss = real_loss + fake_loss\r\n",
    "    return total_disc_loss * 0.5\r\n",
    "\r\n",
    "def generator_loss(fake):\r\n",
    "    return loss_obj(tf.ones_like(fake), fake)\r\n",
    "\r\n",
    "def L1_loss(img_A, img_B):\r\n",
    "    return tf.reduce_mean(tf.abs(img_A - img_B))\r\n",
    "\r\n",
    "def L2_loss(img_A, img_B):\r\n",
    "    diff = tf.abs(img_A - img_B)\r\n",
    "    return tf.reduce_mean(diff*diff)"
   ],
   "outputs": [],
   "metadata": {
    "id": "4kdyuY7fB_P_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Checkpoints"
   ],
   "metadata": {
    "id": "GoC-wcuUDkab"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "identifier = \"20210927_unet_dist_gen_10\"\r\n",
    "checkpoint_path = parent_dir + \"cycle_gan_results/checkpoints/checkpoint_{}\".format(identifier)\r\n",
    "log_data_fn = \"{}/data_{}.p\".format(checkpoint_path, identifier)\r\n",
    "\r\n",
    "ckpt = tf.train.Checkpoint(generator_exp=generator_exp,\r\n",
    "                           generator_sim=generator_sim,\r\n",
    "                           discriminator_sim=discriminator_sim,\r\n",
    "                           discriminator_exp=discriminator_exp,\r\n",
    "                           generator_exp_optimizer=generator_exp_optimizer,\r\n",
    "                           generator_sim_optimizer=generator_sim_optimizer,\r\n",
    "                           discriminator_sim_optimizer=discriminator_sim_optimizer,\r\n",
    "                           discriminator_exp_optimizer=discriminator_exp_optimizer)\r\n",
    "\r\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\r\n",
    "\r\n",
    "# if a checkpoint exists, restore the latest checkpoint.\r\n",
    "if ckpt_manager.latest_checkpoint:\r\n",
    "    #cpath = ckpt_manager.checkpoints[-2]\r\n",
    "    cpath = ckpt_manager.latest_checkpoint\r\n",
    "    print(\"loading checkpoint \", cpath)\r\n",
    "    ckpt.restore(cpath)\r\n",
    "    print ('Latest checkpoint restored!')"
   ],
   "outputs": [],
   "metadata": {
    "id": "6OkV7fNPDSzE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Functions"
   ],
   "metadata": {
    "id": "UTloK0v6Dqjs"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@tf.function\r\n",
    "def train_generators(real_sim, real_exp, train=True):\r\n",
    "  # persistent is set to True because the tape is used more than\r\n",
    "  # once to calculate the gradients.\r\n",
    "  with tf.GradientTape(persistent=True) as tape:\r\n",
    "    # Generator G translates X -> Y\r\n",
    "    # Generator F translates Y -> X.\r\n",
    "    \r\n",
    "    fake_exp   = generator_exp(real_sim, training=train)\r\n",
    "    cycled_sim = generator_sim(fake_exp, training=train)\r\n",
    "\r\n",
    "    fake_sim   = generator_sim(real_exp, training=train)\r\n",
    "    cycled_exp = generator_exp(fake_sim, training=train)\r\n",
    "\r\n",
    "    disc_fake_sim = discriminator_sim(fake_sim, training=False)\r\n",
    "    disc_fake_exp = discriminator_exp(fake_exp, training=False)\r\n",
    "\r\n",
    "    # calculate the loss\r\n",
    "    gen_exp_loss = generator_loss(disc_fake_exp)\r\n",
    "    gen_sim_loss = generator_loss(disc_fake_sim)\r\n",
    "\r\n",
    "    cycle_exp_loss = L1_loss(real_exp, cycled_exp)\r\n",
    "    cycle_sim_loss = L1_loss(real_sim, cycled_sim)\r\n",
    "    total_cycle_loss = cycle_exp_loss + cycle_sim_loss\r\n",
    "\r\n",
    "    dist_gen_exp_loss = L1_loss(real_sim, fake_exp)\r\n",
    "    dist_gen_sim_loss = L1_loss(real_exp, fake_sim)\r\n",
    "\r\n",
    "    # Total generator loss = adversarial loss + cycle loss\r\n",
    "    total_gen_exp_loss = gen_exp_loss + LAMBDA*total_cycle_loss + 0.1*LAMBDA*dist_gen_exp_loss\r\n",
    "    total_gen_sim_loss = gen_sim_loss + LAMBDA*total_cycle_loss + 0.1*LAMBDA*dist_gen_sim_loss\r\n",
    "  \r\n",
    "  if train:\r\n",
    "    # Calculate the gradients for generator and discriminator\r\n",
    "    generator_exp_gradients = tape.gradient(total_gen_exp_loss, generator_exp.trainable_variables)\r\n",
    "    generator_sim_gradients = tape.gradient(total_gen_sim_loss, generator_sim.trainable_variables)\r\n",
    "    \r\n",
    "    # Apply the gradients to the optimizer\r\n",
    "    generator_exp_optimizer.apply_gradients(zip(generator_exp_gradients, generator_exp.trainable_variables))\r\n",
    "    generator_sim_optimizer.apply_gradients(zip(generator_sim_gradients, generator_sim.trainable_variables))\r\n",
    "\r\n",
    "  return cycle_exp_loss, cycle_sim_loss, gen_exp_loss, gen_sim_loss, dist_gen_exp_loss, dist_gen_sim_loss\r\n",
    "\r\n",
    "@tf.function\r\n",
    "def train_discriminators(real_sim, real_exp, train=True):\r\n",
    "  # persistent is set to True because the tape is used more than\r\n",
    "  # once to calculate the gradients.\r\n",
    "  with tf.GradientTape(persistent=True) as tape:\r\n",
    "    # Generator G translates X -> Y\r\n",
    "    # Generator F translates Y -> X.\r\n",
    "    \r\n",
    "    fake_exp   = generator_exp(real_sim, training=train)\r\n",
    "    fake_sim   = generator_sim(real_exp, training=train)\r\n",
    "\r\n",
    "    disc_real_sim = discriminator_sim(real_sim, training=train)\r\n",
    "    disc_fake_sim = discriminator_sim(fake_sim, training=train)\r\n",
    "\r\n",
    "    disc_real_exp = discriminator_exp(real_exp, training=train)\r\n",
    "    disc_fake_exp = discriminator_exp(fake_exp, training=train)\r\n",
    "    \r\n",
    "    # Calculate loss\r\n",
    "    disc_sim_loss = discriminator_loss(disc_real_sim, disc_fake_sim)\r\n",
    "    disc_exp_loss = discriminator_loss(disc_real_exp, disc_fake_exp)\r\n",
    "  \r\n",
    "  if train:\r\n",
    "    # Calculate the gradients for generator and discriminator\r\n",
    "    discriminator_sim_gradients = tape.gradient(disc_sim_loss, discriminator_sim.trainable_variables)\r\n",
    "    discriminator_exp_gradients = tape.gradient(disc_exp_loss, discriminator_exp.trainable_variables)\r\n",
    "  \r\n",
    "    # Apply the gradients to the optimizer\r\n",
    "    discriminator_sim_optimizer.apply_gradients(zip(discriminator_sim_gradients, discriminator_sim.trainable_variables))\r\n",
    "    discriminator_exp_optimizer.apply_gradients(zip(discriminator_exp_gradients, discriminator_exp.trainable_variables))\r\n",
    "  return disc_sim_loss, disc_exp_loss"
   ],
   "outputs": [],
   "metadata": {
    "id": "TSQH9KNLDxx5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "id": "9g2rXhdGeI1E"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gen_exp_losses,   gen_sim_losses   = [], []\r\n",
    "disc_exp_losses,  disc_sim_losses  = [], []\r\n",
    "cycle_exp_losses, cycle_sim_losses = [], []\r\n",
    "dist_gen_exp_losses, dist_gen_sim_losses = [], []\r\n",
    "disc_exp_accs,    disc_sim_accs    = [], []\r\n",
    "epoch = 0\r\n",
    "batch_size = 32\r\n",
    "train_size = 10\r\n",
    "total_epochs = 200\r\n",
    "epoch_step   = 100\r\n",
    "\r\n",
    "try:\r\n",
    "    ( gen_exp_losses,   gen_sim_losses, \r\n",
    "     disc_exp_losses,  disc_sim_losses, \r\n",
    "    cycle_exp_losses, cycle_sim_losses,\r\n",
    "    dist_gen_exp_losses, dist_gen_sim_losses,\r\n",
    "     disc_exp_accs,    disc_sim_accs,      \r\n",
    "     epoch) = pickle.load(open(log_data_fn, \"rb\" ))\r\n",
    "    print(\"loading at epoch \", epoch)\r\n",
    "except Exception as e:\r\n",
    "    print(e)\r\n",
    "    pass\r\n",
    "last=None"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQA1sR3ZinsW",
    "outputId": "34b9bfb0-6716-4dd1-e8d4-9b5ef558cdc9"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "while epoch < total_epochs:\r\n",
    "    start = time.time()\r\n",
    "    lr = lr0*(1 - (epoch - epoch_step)/(total_epochs-1 - epoch_step)) if epoch >= epoch_step else lr0\r\n",
    "    print(epoch, lr)\r\n",
    "    generator_exp_optimizer.lr.assign(lr/(1+2.25*LAMBDA))\r\n",
    "    generator_sim_optimizer.lr.assign(lr/(1+2.25*LAMBDA))\r\n",
    "    discriminator_sim_optimizer.lr.assign(lr)\r\n",
    "    discriminator_exp_optimizer.lr.assign(lr)\r\n",
    "\r\n",
    "    data_sim = glob('{}/*.*'.format(sim_dir))\r\n",
    "    data_exp = glob('{}/*.*'.format(exp_dir))\r\n",
    "    np.random.shuffle(data_sim)\r\n",
    "    np.random.shuffle(data_exp)\r\n",
    "    batch_idxs = min(min(len(data_sim), len(data_exp)), train_size) // batch_size\r\n",
    "\r\n",
    "    dis_losses, gen_losses = np.zeros(2), np.zeros(6)\r\n",
    "    for idx in range(0, batch_idxs):\r\n",
    "        batch_start =  time.time()\r\n",
    "        sim_fn_list = data_sim[idx*batch_size:(idx + 1)*batch_size]\r\n",
    "        exp_fn_list = data_exp[idx*batch_size:(idx + 1)*batch_size]\r\n",
    "\r\n",
    "        real_sim = np.array([load_train_data(fn, num_channels) for fn in sim_fn_list]).astype(np.float32)\r\n",
    "        real_exp = np.array([load_train_data(fn, num_channels) for fn in exp_fn_list]).astype(np.float32)\r\n",
    "\r\n",
    "        dis_losses += train_discriminators(real_sim, real_exp, train=True)\r\n",
    "        gen_losses += train_generators(real_sim, real_exp, train=True)\r\n",
    "\r\n",
    "        print(\"[{}/{}]: {}\".format(idx, batch_idxs, (time.time()-batch_start)/batch_size))\r\n",
    "\r\n",
    "    clear_output(wait=True)\r\n",
    "\r\n",
    "    cycle_exp_loss, cycle_sim_loss, gen_exp_loss, gen_sim_loss, dist_gen_exp_loss, dist_gen_sim_loss= gen_losses/batch_idxs\r\n",
    "    disc_sim_loss, disc_exp_loss = dis_losses/batch_idxs\r\n",
    "\r\n",
    "    # Calculate accuracy\r\n",
    "    fake_sim = generator_sim(real_exp, training=False)\r\n",
    "    fake_exp = generator_exp(real_sim, training=False)\r\n",
    "\r\n",
    "    disc_sim_acc = get_discriminator_acc(discriminator_sim, real_sim, fake_sim, threshold=0.5)\r\n",
    "    disc_exp_acc = get_discriminator_acc(discriminator_exp, real_exp, fake_exp, threshold=0.5)\r\n",
    "\r\n",
    "    gen_exp_losses.append(gen_exp_loss)\r\n",
    "    gen_sim_losses.append(gen_sim_loss)\r\n",
    "    cycle_exp_losses.append(cycle_exp_loss)\r\n",
    "    cycle_sim_losses.append(cycle_sim_loss)\r\n",
    "    dist_gen_exp_losses.append(dist_gen_exp_loss)\r\n",
    "    dist_gen_sim_losses.append(dist_gen_sim_loss)\r\n",
    "    disc_sim_losses.append(disc_sim_loss)\r\n",
    "    disc_exp_losses.append(disc_exp_loss)\r\n",
    "    disc_sim_accs.append(disc_sim_acc)\r\n",
    "    disc_exp_accs.append(disc_exp_acc)\r\n",
    "    epoch += 1\r\n",
    "\r\n",
    "    sample_sim = real_sim[0].reshape([1,fine_size,fine_size,num_channels])\r\n",
    "    sample_exp = real_exp[0].reshape([1,fine_size,fine_size,1])\r\n",
    "    generate_images(generator_exp, generator_sim, sample_sim)\r\n",
    "    generate_images(generator_sim, generator_exp, sample_exp)\r\n",
    "    generate_losses(gen_exp_losses,   gen_sim_losses,\r\n",
    "                    cycle_exp_losses, cycle_sim_losses,\r\n",
    "                    dist_gen_exp_losses, dist_gen_sim_losses,\r\n",
    "                    disc_sim_losses,  disc_exp_losses,   epoch, last=last)\r\n",
    "    generate_accuracies(disc_sim_accs, disc_exp_accs, epoch, last=last)\r\n",
    "\r\n",
    "    if (epoch) % 5 == 0:\r\n",
    "        ckpt_save_path = ckpt_manager.save()\r\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch, ckpt_save_path))\r\n",
    "        pickle.dump( (    gen_exp_losses,   gen_sim_losses, \r\n",
    "                         disc_exp_losses,  disc_sim_losses, \r\n",
    "                        cycle_exp_losses, cycle_sim_losses,\r\n",
    "                        dist_gen_exp_losses, dist_gen_sim_losses,\r\n",
    "                         disc_exp_accs,    disc_sim_accs,      \r\n",
    "                        epoch-1),\r\n",
    "                     open( log_data_fn, \"wb\" ) )\r\n",
    "\r\n",
    "    print ('Time taken for epoch {} is {} sec\\n'.format(epoch, time.time()-start))"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MmLPsIW9EnaL",
    "outputId": "58960db6-4651-430e-d171-7a94029eb170"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "i = 2\r\n",
    "fn_list = [x for x in os.listdir(sim_dir) if \".tif\" in x]\r\n",
    "fn = fn_list[i]\r\n",
    "print(fn)\r\n",
    "img = load_train_data(os.path.join(sim_dir,fn))\r\n",
    "for i in range(num_channels):\r\n",
    "    plt.figure()\r\n",
    "    plt.imshow(img[:,:,i])\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "mlshv8r171WP",
    "U14mcm_gB6jL",
    "GoC-wcuUDkab",
    "G7-L7isOeAW3",
    "UTloK0v6Dqjs"
   ],
   "machine_shape": "hm",
   "name": "continuous cycleGAN for STEM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "interpreter": {
   "hash": "f914bd1de16c1cf64603350c7875757b86e1be9b95a8f8e4031868a367954dd5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}