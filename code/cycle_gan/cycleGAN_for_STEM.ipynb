{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I62chQLHhFzk"
   },
   "source": [
    "# cycleGANs for STEM images\n",
    "\n",
    "In this section, we'll train a GAN to noise up a simulated image to look like an experimental image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iq_MGv7sdeEI"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hsC0wWlu7QM0",
    "outputId": "24183030-eb8b-44fa-a2d6-25b490963e80"
   },
   "outputs": [],
   "source": [
    "from make_dataset import parse_and_save_dir, load_train_data\n",
    "from models import unet_generator, discriminator, generator_resnet\n",
    "import tensorflow as tf\n",
    "from plotting_tools import *\n",
    "import pickle\n",
    "import time\n",
    "from glob import glob\n",
    "from IPython.display import clear_output\n",
    "import scipy.fftpack as sfft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "parent_dir = \"D:/stem-learning/\"\n",
    "exp_dir_raw = parent_dir + \"data/WSe/data_for_gan/experiment/real_107_raw/\"\n",
    "exp_dir = exp_dir_raw[:-1] + \"_normalized/\"\n",
    "fine_size, stride = 1024, 1024\n",
    "num_channels=1\n",
    "parse_and_save_dir(exp_dir_raw, exp_dir, fine_size, stride)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJk0uCGsjHXo"
   },
   "source": [
    "## Loading in Images\n",
    "First, we load in large simulated image and experimental images, and cut them into 256x256 chunks. We then store the images in folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = \"D:/stem-learning/\"\n",
    "exp_dir_raw = parent_dir + \"data/WSe/data_for_gan/experiment/real_all_normalized/\"\n",
    "sim_dir_raw = parent_dir + \"data/WSe/data_for_gan/simulation/sim_pristine/\"\n",
    "\n",
    "gaussian = 0.1\n",
    "\n",
    "exp_dir = exp_dir_raw[:-1] + \"_256_slices/\"\n",
    "sim_dir = sim_dir_raw[:-1] + \"_256_slices/\"\n",
    "\n",
    "\n",
    "\n",
    "fine_size, stride = 256, 256\n",
    "num_channels=1\n",
    "\n",
    "identifier = \"20220530_MODEL_unet_dist_gen_fft_10_SIM_pristine_gaussian_{}_EXP_all\".format(gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_and_save_dir(exp_dir_raw, exp_dir, fine_size, stride)\n",
    "parse_and_save_dir(sim_dir_raw, sim_dir, fine_size, stride, num_channels=num_channels, gaussian=gaussian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlshv8r171WP"
   },
   "source": [
    "## Construct Generator and Discriminator\n",
    "Here, we can choose between a unet generator or a resnet generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6bX45FX75l3"
   },
   "outputs": [],
   "source": [
    "generator_exp = unet_generator(num_channels, 1, \"instancenorm\")\n",
    "generator_sim = unet_generator(1, num_channels, \"instancenorm\")\n",
    "\n",
    "#generator_exp = generator_resnet(64, num_channels, 1)\n",
    "#generator_sim = generator_resnet(64, 1, num_channels)\n",
    "\n",
    "discriminator_sim = discriminator(\"instancenorm\", num_channels)\n",
    "discriminator_exp = discriminator(\"instancenorm\", 1)\n",
    "\n",
    "discriminator_sim_fft = discriminator(\"instancenorm\", num_channels)\n",
    "discriminator_exp_fft = discriminator(\"instancenorm\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr0 = 2e-3\n",
    "generator_exp_optimizer = tf.keras.optimizers.Adam(lr0)\n",
    "generator_sim_optimizer = tf.keras.optimizers.Adam(lr0)\n",
    "\n",
    "discriminator_sim_optimizer = tf.keras.optimizers.Adam(lr0)\n",
    "discriminator_exp_optimizer = tf.keras.optimizers.Adam(lr0)\n",
    "\n",
    "discriminator_sim_fft_optimizer = tf.keras.optimizers.Adam(lr0)\n",
    "discriminator_exp_fft_optimizer = tf.keras.optimizers.Adam(lr0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U14mcm_gB6jL"
   },
   "source": [
    "## Loss Functions and Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kdyuY7fB_P_"
   },
   "outputs": [],
   "source": [
    "LAMBDA = 10\n",
    "#loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "loss_obj = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "def discriminator_loss(real, fake):\n",
    "    real_loss = loss_obj( tf.ones_like(real), real)\n",
    "    fake_loss = loss_obj(tf.zeros_like(fake), fake)\n",
    "    total_disc_loss = (real_loss*len(real) + fake_loss*len(fake))/(len(real + len(fake)))\n",
    "    return total_disc_loss\n",
    "\n",
    "def generator_loss(fake):\n",
    "    return loss_obj(tf.ones_like(fake), fake)\n",
    "\n",
    "def L1_loss(img_A, img_B):\n",
    "    return tf.reduce_mean(tf.abs(img_A - img_B))\n",
    "\n",
    "def L2_loss(img_A, img_B):\n",
    "    diff = tf.abs(img_A - img_B)\n",
    "    return tf.reduce_mean(diff*diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GoC-wcuUDkab"
   },
   "source": [
    "## Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6OkV7fNPDSzE"
   },
   "outputs": [],
   "source": [
    "\n",
    "checkpoint_path = parent_dir + \"cycle_gan_results/checkpoints/checkpoint_{}\".format(identifier)\n",
    "log_data_fn = \"{}/data_{}.p\".format(checkpoint_path, identifier)\n",
    "\n",
    "ckpt = tf.train.Checkpoint(generator_exp=generator_exp,\n",
    "                           generator_sim=generator_sim,\n",
    "                           discriminator_sim=discriminator_sim,\n",
    "                           discriminator_exp=discriminator_exp,\n",
    "                           discriminator_sim_fft=discriminator_sim_fft,\n",
    "                           discriminator_exp_fft=discriminator_exp_fft,\n",
    "                           generator_exp_optimizer=generator_exp_optimizer,\n",
    "                           generator_sim_optimizer=generator_sim_optimizer,\n",
    "                           discriminator_sim_optimizer=discriminator_sim_optimizer,\n",
    "                           discriminator_exp_optimizer=discriminator_exp_optimizer,\n",
    "                           discriminator_sim_fft_optimizer=discriminator_sim_fft_optimizer,\n",
    "                           discriminator_exp_fft_optimizer=discriminator_exp_fft_optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    #cpath = ckpt_manager.checkpoints[-2]\n",
    "    cpath = ckpt_manager.latest_checkpoint\n",
    "    print(\"loading checkpoint \", cpath)\n",
    "    ckpt.restore(cpath)\n",
    "    print ('Latest checkpoint restored!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTloK0v6Dqjs"
   },
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TSQH9KNLDxx5"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_logf2(img_arr):\n",
    "    fft_list = tf.signal.fft2d(tf.cast(tf.reshape(img_arr, [-1, fine_size, fine_size]), tf.complex64))\n",
    "    re_list, im_list = tf.math.real(fft_list), tf.math.imag(fft_list)\n",
    "    f2_list = tf.math.multiply(re_list, re_list) + tf.math.multiply(im_list, im_list)\n",
    "    return tf.reshape(tf.math.log(tf.clip_by_value(f2_list, 1e-6, 1e36)), [-1, fine_size, fine_size, 1])\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_generators(real_sim, real_exp, train=True):\n",
    "    # persistent is set to True because the tape is used more than\n",
    "    # once to calculate the gradients.\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "    # Generator G translates X -> Y\n",
    "    # Generator F translates Y -> X.\n",
    "\n",
    "        #Generate fake and cycled images\n",
    "        fake_exp   = generator_exp(real_sim, training=train)\n",
    "        cycled_sim = generator_sim(fake_exp, training=train)\n",
    "\n",
    "        fake_sim   = generator_sim(real_exp, training=train)\n",
    "        cycled_exp = generator_exp(fake_sim, training=train)\n",
    "\n",
    "        #discriminate images\n",
    "        disc_fake_sim = discriminator_sim(fake_sim, training=False)\n",
    "        disc_fake_exp = discriminator_exp(fake_exp, training=False)\n",
    "\n",
    "        #create fft of images\n",
    "        fake_sim_fft = get_logf2(fake_sim)\n",
    "        fake_exp_fft = get_logf2(fake_exp)\n",
    "\n",
    "        #discriminate ffts\n",
    "        disc_fake_sim_fft = discriminator_sim_fft(fake_sim_fft, training=False)\n",
    "        disc_fake_exp_fft = discriminator_exp_fft(fake_exp_fft, training=False)\n",
    "\n",
    "        # calculate the loss\n",
    "        gen_exp_loss = (generator_loss(disc_fake_exp) + generator_loss(disc_fake_exp_fft))/2\n",
    "        gen_sim_loss = (generator_loss(disc_fake_sim) + generator_loss(disc_fake_sim_fft))/2\n",
    "\n",
    "        cycle_exp_loss = L1_loss(real_exp, cycled_exp)\n",
    "        cycle_sim_loss = L1_loss(real_sim, cycled_sim)\n",
    "        total_cycle_loss = (cycle_exp_loss + cycle_sim_loss)/2\n",
    "\n",
    "        dist_gen_exp_loss = L2_loss(real_sim, fake_exp)\n",
    "        dist_gen_sim_loss = L2_loss(real_exp, fake_sim)\n",
    "\n",
    "        # Total generator loss = adversarial loss + cycle loss\n",
    "        total_gen_exp_loss = (gen_exp_loss + LAMBDA*total_cycle_loss + 0.1*LAMBDA*dist_gen_exp_loss)/(1 + 1.1*LAMBDA) #\n",
    "        total_gen_sim_loss = (gen_sim_loss + LAMBDA*total_cycle_loss + 0.1*LAMBDA*dist_gen_sim_loss)/(1 + 1.1*LAMBDA) #\n",
    "\n",
    "    if train:\n",
    "        # Calculate the gradients for generator and discriminator\n",
    "        generator_exp_gradients = tape.gradient(total_gen_exp_loss, generator_exp.trainable_variables)\n",
    "        generator_sim_gradients = tape.gradient(total_gen_sim_loss, generator_sim.trainable_variables)\n",
    "\n",
    "        # Apply the gradients to the optimizer\n",
    "        generator_exp_optimizer.apply_gradients(zip(generator_exp_gradients, generator_exp.trainable_variables))\n",
    "        generator_sim_optimizer.apply_gradients(zip(generator_sim_gradients, generator_sim.trainable_variables))\n",
    "\n",
    "    return cycle_exp_loss, cycle_sim_loss, gen_exp_loss, gen_sim_loss, dist_gen_exp_loss, dist_gen_sim_loss\n",
    "\n",
    "@tf.function\n",
    "def train_discriminators(real_sim, real_exp, train=True):\n",
    "    # persistent is set to True because the tape is used more than\n",
    "    # once to calculate the gradients.\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Generator G translates X -> Y\n",
    "        # Generator F translates Y -> X.\n",
    "    \n",
    "        #generate fake images\n",
    "        fake_exp   = generator_exp(real_sim, training=train)\n",
    "        fake_sim   = generator_sim(real_exp, training=train)\n",
    "\n",
    "        #create ffts\n",
    "        fake_exp_fft = get_logf2(fake_exp)\n",
    "        fake_sim_fft = get_logf2(fake_sim)\n",
    "        real_exp_fft = get_logf2(real_exp)\n",
    "        real_sim_fft = get_logf2(real_sim)\n",
    "\n",
    "        #discriminate images\n",
    "        disc_real_sim = discriminator_sim(real_sim, training=train)\n",
    "        disc_fake_sim = discriminator_sim(fake_sim, training=train)\n",
    "\n",
    "        disc_real_exp = discriminator_exp(real_exp, training=train)\n",
    "        disc_fake_exp = discriminator_exp(fake_exp, training=train)\n",
    "\n",
    "        #discriminate ffts\n",
    "        disc_real_sim_fft = discriminator_sim_fft(real_sim_fft, training=train)\n",
    "        disc_fake_sim_fft = discriminator_sim_fft(fake_sim_fft, training=train)\n",
    "\n",
    "        disc_real_exp_fft = discriminator_exp_fft(real_exp_fft, training=train)\n",
    "        disc_fake_exp_fft = discriminator_exp_fft(fake_exp_fft, training=train)\n",
    "\n",
    "        # Calculate loss\n",
    "        disc_sim_loss = discriminator_loss(disc_real_sim, disc_fake_sim)\n",
    "        disc_exp_loss = discriminator_loss(disc_real_exp, disc_fake_exp)\n",
    "\n",
    "        disc_sim_loss_fft = discriminator_loss(disc_real_sim_fft, disc_fake_sim_fft)\n",
    "        disc_exp_loss_fft = discriminator_loss(disc_real_exp_fft, disc_fake_exp_fft)\n",
    "        \n",
    "    if train:\n",
    "        # Calculate the gradients for generator and discriminator\n",
    "        discriminator_sim_gradients = tape.gradient(disc_sim_loss, discriminator_sim.trainable_variables)\n",
    "        discriminator_exp_gradients = tape.gradient(disc_exp_loss, discriminator_exp.trainable_variables)\n",
    "\n",
    "        discriminator_sim_gradients_fft = tape.gradient(disc_sim_loss_fft, discriminator_sim_fft.trainable_variables)\n",
    "        discriminator_exp_gradients_fft = tape.gradient(disc_exp_loss_fft, discriminator_exp_fft.trainable_variables)\n",
    "\n",
    "        # Apply the gradients to the optimizer\n",
    "        discriminator_sim_optimizer.apply_gradients(zip(discriminator_sim_gradients, \n",
    "                                                        discriminator_sim.trainable_variables))\n",
    "        discriminator_exp_optimizer.apply_gradients(zip(discriminator_exp_gradients, \n",
    "                                                        discriminator_exp.trainable_variables))\n",
    "\n",
    "        discriminator_sim_fft_optimizer.apply_gradients(zip(discriminator_sim_gradients_fft, \n",
    "                                                            discriminator_sim_fft.trainable_variables))\n",
    "        discriminator_exp_fft_optimizer.apply_gradients(zip(discriminator_exp_gradients_fft, \n",
    "                                                            discriminator_exp_fft.trainable_variables))\n",
    "\n",
    "    return disc_sim_loss, disc_exp_loss, disc_sim_loss_fft, disc_exp_loss_fft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9g2rXhdGeI1E"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQA1sR3ZinsW",
    "outputId": "34b9bfb0-6716-4dd1-e8d4-9b5ef558cdc9"
   },
   "outputs": [],
   "source": [
    "gen_exp_losses,   gen_sim_losses   = [], []\n",
    "disc_exp_losses,  disc_sim_losses  = [], []\n",
    "disc_exp_fft_losses,  disc_sim_fft_losses  = [], []\n",
    "cycle_exp_losses, cycle_sim_losses = [], []\n",
    "dist_gen_exp_losses, dist_gen_sim_losses = [], []\n",
    "disc_exp_accs,    disc_sim_accs    = [], []\n",
    "epoch = 0\n",
    "batch_size = 32\n",
    "train_size = 100000\n",
    "total_epochs = 200\n",
    "epoch_step   = 100\n",
    "\n",
    "try:\n",
    "    ( gen_exp_losses,   gen_sim_losses, \n",
    "     disc_exp_losses,  disc_sim_losses, \n",
    "     disc_exp_fft_losses,  disc_sim_fft_losses, \n",
    "    cycle_exp_losses, cycle_sim_losses,\n",
    "    dist_gen_exp_losses, dist_gen_sim_losses,\n",
    "     disc_exp_accs,    disc_sim_accs,      \n",
    "     epoch) = pickle.load(open(log_data_fn, \"rb\" ))\n",
    "    print(\"loading at epoch \", epoch)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pass\n",
    "last=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MmLPsIW9EnaL",
    "outputId": "58960db6-4651-430e-d171-7a94029eb170",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_sim = glob('{}/*.*'.format(sim_dir))\n",
    "data_exp = glob('{}/*.*'.format(exp_dir))\n",
    "\n",
    "batch_idxs = min(min(len(data_sim), len(data_exp)), train_size) // batch_size\n",
    "print(\"batch_idxs = \", batch_idxs)\n",
    "\n",
    "while epoch < total_epochs:\n",
    "    start = time.time()\n",
    "    lr = lr0*(1 - (epoch - epoch_step)/(total_epochs-1 - epoch_step)) if epoch >= epoch_step else lr0\n",
    "    print(epoch, lr)\n",
    "    generator_exp_optimizer.lr.assign(lr)\n",
    "    generator_sim_optimizer.lr.assign(lr)\n",
    "    discriminator_sim_optimizer.lr.assign(lr)\n",
    "    discriminator_exp_optimizer.lr.assign(lr)\n",
    "    discriminator_sim_fft_optimizer.lr.assign(lr)\n",
    "    discriminator_exp_fft_optimizer.lr.assign(lr)\n",
    "\n",
    "    np.random.shuffle(data_sim)\n",
    "    np.random.shuffle(data_exp)\n",
    "\n",
    "    dis_losses, gen_losses = np.zeros(4), np.zeros(6)\n",
    "    for idx in range(0, batch_idxs):\n",
    "        batch_start =  time.time()\n",
    "        sim_fn_list = data_sim[idx*batch_size:(idx + 1)*batch_size]\n",
    "        exp_fn_list = data_exp[idx*batch_size:(idx + 1)*batch_size]\n",
    "\n",
    "        real_sim = np.array([load_train_data(fn, num_channels) for fn in sim_fn_list]).astype(np.float32)\n",
    "        real_exp = np.array([load_train_data(fn, num_channels) for fn in exp_fn_list]).astype(np.float32)\n",
    "\n",
    "        dis_losses += train_discriminators(real_sim, real_exp, train=True)\n",
    "        gen_losses += train_generators(real_sim, real_exp, train=True)\n",
    "\n",
    "        print(\"[{}/{}]: {}\".format(idx, batch_idxs, (time.time()-batch_start)/batch_size))\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    cycle_exp_loss, cycle_sim_loss, gen_exp_loss, gen_sim_loss, dist_gen_exp_loss, dist_gen_sim_loss= gen_losses/batch_idxs\n",
    "    disc_sim_loss, disc_exp_loss, disc_sim_loss_fft, disc_exp_loss_fft = dis_losses/batch_idxs\n",
    "\n",
    "    # Calculate accuracy\n",
    "    fake_sim = generator_sim(real_exp, training=False)\n",
    "    fake_exp = generator_exp(real_sim, training=False)\n",
    "\n",
    "    disc_sim_acc = get_discriminator_acc(discriminator_sim, real_sim, fake_sim, threshold=0.5)\n",
    "    disc_exp_acc = get_discriminator_acc(discriminator_exp, real_exp, fake_exp, threshold=0.5)\n",
    "\n",
    "    gen_exp_losses.append(gen_exp_loss)\n",
    "    gen_sim_losses.append(gen_sim_loss)\n",
    "    cycle_exp_losses.append(cycle_exp_loss)\n",
    "    cycle_sim_losses.append(cycle_sim_loss)\n",
    "    dist_gen_exp_losses.append(dist_gen_exp_loss)\n",
    "    dist_gen_sim_losses.append(dist_gen_sim_loss)\n",
    "    disc_sim_losses.append(disc_sim_loss)\n",
    "    disc_exp_losses.append(disc_exp_loss)\n",
    "    disc_sim_fft_losses.append(disc_sim_loss_fft)\n",
    "    disc_exp_fft_losses.append(disc_exp_loss_fft)\n",
    "    disc_sim_accs.append(disc_sim_acc)\n",
    "    disc_exp_accs.append(disc_exp_acc)\n",
    "    epoch += 1\n",
    "\n",
    "    sample_sim = real_sim[0].reshape([1,fine_size,fine_size,num_channels])\n",
    "    sample_exp = real_exp[0].reshape([1,fine_size,fine_size,1])\n",
    "    generate_images(generator_exp, generator_sim, sample_sim)\n",
    "    generate_images(generator_sim, generator_exp, sample_exp)\n",
    "    generate_losses(gen_exp_losses,   gen_sim_losses,\n",
    "                    cycle_exp_losses, cycle_sim_losses,\n",
    "                    dist_gen_exp_losses, dist_gen_sim_losses,\n",
    "                    disc_sim_losses,  disc_exp_losses,   epoch, last=last)\n",
    "    generate_accuracies(disc_sim_accs, disc_exp_accs, epoch, last=last)\n",
    "\n",
    "    if (epoch) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch, ckpt_save_path))\n",
    "        pickle.dump( (    gen_exp_losses,   gen_sim_losses, \n",
    "                         disc_exp_losses,  disc_sim_losses,\n",
    "                      disc_exp_fft_losses,  disc_sim_fft_losses,\n",
    "                        cycle_exp_losses, cycle_sim_losses,\n",
    "                        dist_gen_exp_losses, dist_gen_sim_losses,\n",
    "                         disc_exp_accs,    disc_sim_accs,      \n",
    "                        epoch-1),\n",
    "                     open( log_data_fn, \"wb\" ) )\n",
    "\n",
    "    print ('Time taken for epoch {} is {} sec\\n'.format(epoch, time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "mlshv8r171WP",
    "U14mcm_gB6jL",
    "GoC-wcuUDkab",
    "G7-L7isOeAW3",
    "UTloK0v6Dqjs"
   ],
   "machine_shape": "hm",
   "name": "continuous cycleGAN for STEM.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "ec772f830160d69d38705b4523acafc483ac4a24cc29ee6bb51e4e8e36c25f20"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
